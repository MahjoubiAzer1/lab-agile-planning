{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhnEDG2JcgmvTdkx0P+Duz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahjoubiAzer1/lab-agile-planning/blob/main/Memoire_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "IYcWgkf-o1qC",
        "outputId": "8fa5ed75-d543-44e8-f082-de71a1eacc02"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7d15603f2a12>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mvalid_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Enter category for phrase '{phrase}' (0:sport, 1:politics, 2:health, 3:entrepreneurship, 4:others, TITI to stop): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'titi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mvalid_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#this code is used  for  Labilisation Phase\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/ReadyLDA.csv')\n",
        "\n",
        "# Check if \"category\" column exists and remove it if present\n",
        "if 'category' in data.columns:\n",
        "    data = data.drop(columns=['category'])\n",
        "\n",
        "# Define category mappings\n",
        "category_mappings = {\n",
        "    '0': 'sport',\n",
        "    '1': 'politics',\n",
        "    '2': 'health',\n",
        "    '3': 'entrepreneurship',\n",
        "    '4': 'others'\n",
        "}\n",
        "# Iterate over the phrases\n",
        "for index, row in data.iterrows():\n",
        "    phrase = row['phrase']\n",
        "    # Prompt for category input\n",
        "    valid_input = False\n",
        "    while not valid_input:\n",
        "        category = input(f\"Enter category for phrase '{phrase}' (0:sport, 1:politics, 2:health, 3:entrepreneurship, 4:others, TITI to stop): \")\n",
        "        if category.lower() == 'titi':\n",
        "            valid_input = True\n",
        "            break\n",
        "        elif category in category_mappings:\n",
        "            valid_input = True\n",
        "        else:\n",
        "            print(\"Invalid category! Please enter a valid category or 'TITI' to stop.\")\n",
        "\n",
        "    # Check if the user wants to stop\n",
        "    if category.lower() == 'titi':\n",
        "        break\n",
        "\n",
        "    # Assign the category\n",
        "    data.at[index, 'category'] = category_mappings[category]\n",
        "\n",
        "# Write back to the CSV file\n",
        "data.to_csv('updated_file.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"MahjoubiAzer1\"\n",
        "!git config --global user.email \"mahjoubiazer@gmail.com\"\n"
      ],
      "metadata": {
        "id": "Ju8sTPEMxsFg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Define the Git commands\n",
        "commands = [\n",
        "    \"git remote add origin https://github.com/MahjoubiAzer1/Memoire.git\",\n",
        "    \"git branch -M main\",\n",
        "    \"git push -u origin main\"\n",
        "]\n",
        "\n",
        "# Execute the Git commands one by one\n",
        "for command in commands:\n",
        "    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    output, error = process.communicate()\n",
        "\n",
        "    if process.returncode == 0:\n",
        "        print(f\"Command '{command}' executed successfully\")\n",
        "    else:\n",
        "        print(f\"Error executing command '{command}': {error.decode('utf-8')}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipQ7eXj5nxJ",
        "outputId": "c6ce2a03-d964-46b3-bcf8-972d7bf5a32f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error executing command 'git remote add origin https://github.com/MahjoubiAzer1/Memoire.git': fatal: not a git repository (or any of the parent directories): .git\n",
            "\n",
            "Error executing command 'git branch -M main': fatal: not a git repository (or any of the parent directories): .git\n",
            "\n",
            "Error executing command 'git push -u origin main': fatal: not a git repository (or any of the parent directories): .git\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA Model Working        With Evaluation Metrics    accuracy ()  precision    recall  f1-score   support for each topic )\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/updated_file.csv')\n",
        "\n",
        "\n",
        "# Drop rows with missing values\n",
        "data = data.dropna(subset=['phrase', 'category'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data['phrase'])\n",
        "X_test = vectorizer.transform(test_data['phrase'])\n",
        "\n",
        "# Create a dictionary for category labels\n",
        "category_dict = {\n",
        "    'sport': 0,\n",
        "    'politics': 1,\n",
        "    'health': 2,\n",
        "    'entrepreneurship': 3,\n",
        "    'others': 4\n",
        "}\n",
        "\n",
        "# Map phrases to category labels\n",
        "y_train = train_data['category'].map(category_dict)\n",
        "y_test = test_data['category'].map(category_dict)\n",
        "\n",
        "# Check for any remaining NaN values in y_test\n",
        "if y_test.isnull().values.any():\n",
        "    # Fill NaN values with a default label or a specific value of your choice\n",
        "    y_test = y_test.fillna('default')\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_model.fit(X_train)\n",
        "\n",
        "# Predict the categories for the test set\n",
        "y_pred = lda_model.transform(X_test).argmax(axis=1)\n",
        "\n",
        "# Convert the predicted labels back to category names\n",
        "predicted_categories = pd.Series(y_pred).map({v: k for k, v in category_dict.items()})\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"\\nF1-score:\", f1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=category_dict.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-tn3T4E1nvy",
        "outputId": "6a7715ef-7b83-4533-957b-8cc451e34597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[0 0 0 0 0]\n",
            " [1 0 0 0 0]\n",
            " [1 0 0 0 0]\n",
            " [1 3 0 1 0]\n",
            " [1 1 1 0 1]]\n",
            "\n",
            "Accuracy: 0.18181818181818182\n",
            "\n",
            "F1-score: 0.296969696969697\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "           sport       0.00      0.00      0.00         0\n",
            "        politics       0.00      0.00      0.00         1\n",
            "          health       0.00      0.00      0.00         1\n",
            "entrepreneurship       1.00      0.20      0.33         5\n",
            "          others       1.00      0.25      0.40         4\n",
            "\n",
            "        accuracy                           0.18        11\n",
            "       macro avg       0.40      0.09      0.15        11\n",
            "    weighted avg       0.82      0.18      0.30        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HDPModel Working        With Evaluation Metrics    accuracy ()  precision    recall  f1-score   support for each topic )\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from sklearn.decomposition import MiniBatchSparsePCA\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import MiniBatchSparsePCA\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/updated_file.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data = data.dropna(subset=['phrase', 'category'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data['phrase'])\n",
        "X_test = vectorizer.transform(test_data['phrase'])\n",
        "\n",
        "# Create a dictionary for category labels\n",
        "category_dict = {\n",
        "    'sport': 0,\n",
        "    'politics': 1,\n",
        "    'health': 2,\n",
        "    'entrepreneurship': 3,\n",
        "    'others': 4\n",
        "}\n",
        "\n",
        "# Map phrases to category labels\n",
        "y_train = train_data['category'].map(category_dict)\n",
        "y_test = test_data['category'].map(category_dict)\n",
        "\n",
        "# Check for any remaining NaN values in y_test\n",
        "if y_test.isnull().values.any():\n",
        "    # Fill NaN values with a default label or a specific value of your choice\n",
        "    y_test = y_test.fillna('default')\n",
        "\n",
        "# Train the HDP model\n",
        "hdp_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "hdp_model.fit(X_train)\n",
        "\n",
        "# Predict the categories for the test set\n",
        "y_pred = hdp_model.transform(X_test).argmax(axis=1)\n",
        "\n",
        "# Convert the predicted labels back to category names\n",
        "predicted_categories = pd.Series(y_pred).map({v: k for k, v in category_dict.items()})\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"\\nF1-score:\", f1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=category_dict.keys()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL_RD21QveLM",
        "outputId": "d6467492-7406-4948-e478-0e7c2935f4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[0 0 0 0 0]\n",
            " [1 0 0 0 0]\n",
            " [1 0 0 0 0]\n",
            " [1 3 0 1 0]\n",
            " [1 1 1 0 1]]\n",
            "\n",
            "Accuracy: 0.18181818181818182\n",
            "\n",
            "F1-score: 0.296969696969697\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "           sport       0.00      0.00      0.00         0\n",
            "        politics       0.00      0.00      0.00         1\n",
            "          health       0.00      0.00      0.00         1\n",
            "entrepreneurship       1.00      0.20      0.33         5\n",
            "          others       1.00      0.25      0.40         4\n",
            "\n",
            "        accuracy                           0.18        11\n",
            "       macro avg       0.40      0.09      0.15        11\n",
            "    weighted avg       0.82      0.18      0.30        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Translation using google API 10000 by 10000  ( after translation  use fusion algorithm to make on file of the translatoin )\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from googletrans import Translator\n",
        "import os\n",
        "from tqdm import tqdm  # Import the tqdm library\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if the file exists\n",
        "file_path = '/content/drive/MyDrive/d/Samawel.csv'\n",
        "if not os.path.isfile(file_path):\n",
        "    print(\"File not found:\", file_path)\n",
        "else:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Create a translator object\n",
        "    translator = Translator(service_urls=['translate.google.com'])\n",
        "\n",
        "    # Define a function to translate a text from Tunisian Arabic to English\n",
        "    def translate(text):\n",
        "        try:\n",
        "            translation = translator.translate(text, src='ar', dest='en').text\n",
        "        except:\n",
        "            translation = ''\n",
        "        return translation\n",
        "\n",
        "    # Extract the 'text_clean' column as a list of strings\n",
        "    text_list = df['text_clean'].tolist()\n",
        "\n",
        "    batch_size = 1000\n",
        "    start_index = 0\n",
        "\n",
        "    while start_index < len(text_list):\n",
        "        end_index = start_index + batch_size\n",
        "        batch_text = text_list[start_index:end_index]\n",
        "\n",
        "        # Translate the batch of text\n",
        "        translated_texts = []\n",
        "        for text in tqdm(batch_text, desc=\"Translating\"):\n",
        "            translated_texts.append(translate(text))\n",
        "\n",
        "        # Add the translated text to the DataFrame\n",
        "        df_batch = pd.DataFrame({'text_clean': batch_text, 'text_clean_en': translated_texts})\n",
        "        output_file_path = f'/content/drive/MyDrive/Documents/Data_batch_{start_index + 1}.csv'\n",
        "        df_batch.to_csv(output_file_path, index=False)\n",
        "        print(f\"Batch {start_index // batch_size + 1} (lines {start_index + 1}-{end_index}) saved to:\", output_file_path)\n",
        "\n",
        "        start_index = end_index\n",
        "\n",
        "print(\"All translations completed.\")"
      ],
      "metadata": {
        "id": "YgnUcIOb1YT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translation is made 1000 rows per 1000 rows saved in myltiple file   this code willl  merge the  file in the correct order , ( version is not for google collab  run it  on your visual studio code for exmaple  ( order is  garanted by the code  ))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the directory where the CSV files are located\n",
        "directory_path = './'  # Change this to the path of your directory\n",
        "\n",
        "# Generate list of file names\n",
        "file_names = [f\"DATA_Batch_{i}.csv\" for i in range(1001, 200001, 1000)]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each file and append its content to final_df\n",
        "for file_name in file_names:\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        final_df = final_df.append(df)\n",
        "\n",
        "# Write the final DataFrame to an output CSV file\n",
        "output_file_path = os.path.join(directory_path, 'Merged_DATA.csv')\n",
        "final_df.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "cGci7gRFcy4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#translation with openai api (paid)   ( Don't Forget to create you own api key and put it  in  (\"SK-XXXXXXXXXX\")  )  version is  for google collab  ( you can get 18 $ of transaction for free when you enter  payement information )\n",
        "\n",
        "\n",
        "\n",
        " import openai\n",
        "\n",
        "# Define your OpenAI API key\n",
        "api_key = \"YOUR_API_KEY\"\n",
        "\n",
        "# Initialize the OpenAI API client\n",
        "openai.api_key = api_key\n",
        "\n",
        "# Define a function for translation\n",
        "def translate_to_english(text):\n",
        "    # Make a request to the API for translation\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci-codex\",\n",
        "        prompt=f\"Translate the following Tunisian text to English:\\n\\n{text}\",\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=[\"\\n\"]\n",
        "    )\n",
        "\n",
        "    # Extract the translated text from the response\n",
        "    translation = response.choices[0].text.strip()\n",
        "\n",
        "    return translation\n",
        "\n",
        "# Example data\n",
        "data = [\n",
        "    ['tyuy', ''],\n",
        "    ['tres pratique facile', ''],\n",
        "    ['super bien', '']\n",
        "]\n",
        "\n",
        "# Translate each item in the data\n",
        "translated_data = []\n",
        "\n",
        "for item in data:\n",
        "    tunisian_text = item[0]\n",
        "    english_translation = translate_to_english(tunisian_text)\n",
        "    translated_data.append([tunisian_text, english_translation])\n",
        "\n",
        "# Print the translated data\n",
        "for item in translated_data:\n",
        "    print(item)\n",
        "\n"
      ],
      "metadata": {
        "id": "sUVg4B5DdwOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Statistical translation technic\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.translate import IBMModel1\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('comtrans')\n",
        "\n",
        "# Prepare parallel text data\n",
        "bitext = nltk.corpus.comtrans.aligned_sents()\n",
        "\n",
        "# Train IBM Model\n",
        "ibm1 = IBMModel1(bitext, 5)\n",
        "\n",
        "# Define a function for translation\n",
        "def translate_to_english(text):\n",
        "    # Tokenize the input text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Translate using IBM Model 1\n",
        "    translation = ibm1.align(tokens)\n",
        "\n",
        "    # Extract English words from the translation\n",
        "    english_translation = [pair[1] for pair in translation]\n",
        "\n",
        "    return ' '.join(english_translation)\n",
        "\n",
        "# Example data\n",
        "data = [\n",
        "    ['tyuy', ''],\n",
        "    ['tres pratique facile', ''],\n",
        "    ['super bien', '']\n",
        "]\n",
        "\n",
        "# Translate each item in the data\n",
        "translated_data = []\n",
        "\n",
        "for item in data:\n",
        "    tunisian_text = item[0]\n",
        "    english_translation = translate_to_english(tunisian_text)\n",
        "    translated_data.append([tunisian_text, english_translation])\n",
        "\n",
        "# Print the translated data\n",
        "for item in translated_data:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "R_oQvyqAfP72",
        "outputId": "f4dc45b6-6bdc-469f-824b-2413707ef17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package comtrans to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e0289564843f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train IBM Model 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mibm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIBMModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbitext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Define a function for translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/translate/ibm1.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentence_aligned_corpus, iterations, probability_tables)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_aligned_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_aligned_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/translate/ibm1.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, parallel_corpus)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# E step (a): Compute normalization factors to weigh counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtotal_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_all_alignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# E step (b): Collect counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/translate/ibm1.py\u001b[0m in \u001b[0;36mprob_all_alignments\u001b[0;34m(self, src_sentence, trg_sentence)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrg_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0malignment_prob_for_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_alignment_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0malignment_prob_for_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/translate/ibm1.py\u001b[0m in \u001b[0;36mprob_alignment_point\u001b[0;34m(self, s, t)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0malignment_prob_for_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_alignment_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0mProbability\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mword\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mt\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maligned\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this script is used to delete the first column ( which is  the original data and let the translated version to give toi the  process greed search/random search and give to the LDA/HDP models  )\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "import csv\n",
        "\n",
        "# Authenticate and create GoogleDrive instance\n",
        "gauth = GoogleAuth()\n",
        "gauth.LocalWebserverAuth()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        " # Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if the file exists\n",
        "file_path = '/content/drive/MyDrive/d/Samawel.csv'\n",
        "\n",
        "\n",
        "# Read the CSV file and remove the first column\n",
        "with open('input.csv', 'r') as infile, open('output.csv', 'w', newline='') as outfile:\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "    for row in reader:\n",
        "        writer.writerow(row[1:])  # Write all columns except the first one\n",
        "\n",
        "# Upload the modified file back to Google Drive\n",
        "modified_file = drive.CreateFile({'title': 'output.csv'})\n",
        "modified_file.Upload()\n",
        "\n",
        "# Clean up local files\n",
        "os.remove('input.csv')\n",
        "os.remove('output.csv')\n",
        "\n",
        "print(\"First column deleted and file updated on Google Drive.\")\n"
      ],
      "metadata": {
        "id": "XxbtWPn_hH4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EANDOM SEARCH LDA FINDING OPTIMAL PARAMETERS\n",
        " # Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "file_path = \"/content/drive/MyDrive/DATA BATCH 3/Data_batch_4002.csv\"  # Provide the correct path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Use the second column only\n",
        "texts = data['text_clean_en'].dropna().tolist()\n",
        "\n",
        "# Convert text data to a matrix of token counts\n",
        "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Define LDA model and parameters grid for random search\n",
        "lda = LatentDirichletAllocation(learning_method='online')\n",
        "\n",
        "params = {\n",
        "    'n_components': [5, 10, 15, 20],  # or any other values you are interested in\n",
        "    'learning_decay': [0.5, 0.7, 0.9],  # common values for lda\n",
        "    'doc_topic_prior': [None] + list(10.0**(-i) for i in range(5)),  # alpha parameter, including the default value (None)\n",
        "    'topic_word_prior': [None] + list(10.0**(-i) for i in range(5))  # beta parameter, including the default value (None)\n",
        "}\n",
        "\n",
        "# Random Search for best LDA parameters\n",
        "search = RandomizedSearchCV(lda, param_distributions=params, n_iter=30, n_jobs=-1)\n",
        "search.fit(X)\n",
        "\n",
        "print(\"Best Model's Params: \", search.best_params_)\n",
        "print(\"Best Log Likelihood Score: \", search.best_score_)\n",
        "\n",
        "# Fit the best LDA model\n",
        "best_lda = search.best_estimator_\n",
        "best_lda.fit(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "a61S-TiR06CT",
        "outputId": "2b96d1ee-18fe-4447-bbff-b386e80f7133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-267d6052c55c>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Random Search for best LDA parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Model's Params: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #used for deleting lines ( put range if needed )\n",
        "\n",
        "\n",
        " from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your file on Google Drive\n",
        "file_path = '/content/drive/MyDrive/DATA BATCH 3/Data_batch_4002.csv'\n",
        "\n",
        "# Load the CSV into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop lines 2 to 981 (inclusive)\n",
        "df = df.drop(index=range(1, 981))\n",
        "\n",
        "# Save the modified DataFrame back to the CSV\n",
        "df.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "qiKZlOQy4722"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARAMETER TUNNING  GREED SEARCH LDA ( ALPHA AND BETA  )\n",
        "\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "file_path = '/content/drive/MyDrive/DATA BATCH 3/Data_batch_4002.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Use the second column only\n",
        "texts = data['text_clean_en'].dropna().tolist()\n",
        "\n",
        "# Convert text data to a matrix of token counts\n",
        "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Define LDA model and parameters grid for grid search\n",
        "lda = LatentDirichletAllocation(learning_method='online')\n",
        "\n",
        "params = {\n",
        "    'n_components': [5, 10, 15, 20],  # or any other values you are interested in\n",
        "    'learning_decay': [0.5, 0.7, 0.9],  # common values for lda\n",
        "    'doc_topic_prior': [None] + list(10.0**(-i) for i in range(5)),  # alpha parameter, including the default value (None)\n",
        "    'topic_word_prior': [None] + list(10.0**(-i) for i in range(5))  # beta parameter, including the default value (None)\n",
        "}\n",
        "\n",
        "# Grid Search for best LDA parameters\n",
        "search = GridSearchCV(lda, param_grid=params, n_jobs=-1, verbose=1)\n",
        "search.fit(X)\n",
        "\n",
        "print(\"Best Model's Params: \", search.best_params_)\n",
        "print(\"Best Log Likelihood Score: \", search.best_score_)\n",
        "\n",
        "# Fit the best LDA model\n",
        "best_lda = search.best_estimator_\n",
        "best_lda.fit(X)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "mlKmYCWizh7w",
        "outputId": "a2c9a6a0-942e-428c-9bb9-fbd02b48e026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best Model's Params:  {'doc_topic_prior': 1.0, 'learning_decay': 0.9, 'n_components': 5, 'topic_word_prior': 1.0}\n",
            "Best Log Likelihood Score:  -25.463771963859877\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(doc_topic_prior=1.0, learning_decay=0.9,\n",
              "                          learning_method='online', n_components=5,\n",
              "                          topic_word_prior=1.0)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(doc_topic_prior=1.0, learning_decay=0.9,\n",
              "                          learning_method=&#x27;online&#x27;, n_components=5,\n",
              "                          topic_word_prior=1.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(doc_topic_prior=1.0, learning_decay=0.9,\n",
              "                          learning_method=&#x27;online&#x27;, n_components=5,\n",
              "                          topic_word_prior=1.0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM SEARCH HDP MODEL\n",
        " import numpy as np\n",
        "import gensim\n",
        "from gensim.models import HdpModel\n",
        "from gensim.corpora import Dictionary\n",
        "from random import choice\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset from Google Drive\n",
        "data_path = '/content/drive/MyDrive/DATA BATCH 3/Data_batch_4002.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "texts = data['text_clean_en'].dropna().apply(lambda x: x.split()).tolist() # assuming space-separated words\n",
        "\n",
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Hyperparameters to sample from\n",
        "alpha_values = np.linspace(0.1, 2, 10)\n",
        "gamma_values = np.linspace(0.1, 2, 10)\n",
        "\n",
        "best_coherence = -np.inf\n",
        "best_params = None\n",
        "\n",
        "# Random Search\n",
        "for _ in range(20):  # 20 iterations\n",
        "    alpha = choice(alpha_values)\n",
        "    gamma = choice(gamma_values)\n",
        "\n",
        "    model = HdpModel(corpus, dictionary, alpha=alpha, gamma=gamma)\n",
        "    coherence = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
        "\n",
        "    if coherence > best_coherence:\n",
        "        best_coherence = coherence\n",
        "        best_params = (alpha, gamma)\n",
        "\n",
        "print(\"Best Alpha, Gamma:\", best_params)\n",
        "print(\"Best Coherence:\", best_coherence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf2pgF516-T0",
        "outputId": "69e6df85-75cf-4d78-eb67-a4e6e376ec42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Alpha, Gamma: (0.9444444444444444, 1.1555555555555557)\n",
            "Best Coherence: 0.6324767793434374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GRID SEARCH HDP MODEL\n",
        "\n",
        "best_coherence = -np.inf\n",
        "best_params = None\n",
        "\n",
        "# Grid Search\n",
        "for alpha in alpha_values:\n",
        "    for gamma in gamma_values:\n",
        "        model = HdpModel(corpus, dictionary, alpha=alpha, gamma=gamma)\n",
        "        coherence = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
        "\n",
        "        if coherence > best_coherence:\n",
        "            best_coherence = coherence\n",
        "            best_params = (alpha, gamma)\n",
        "\n",
        "print(\"Best Alpha, Gamma:\", best_params)\n",
        "print(\"Best Coherence:\", best_coherence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgsoCb1O78kM",
        "outputId": "938e7ea0-f056-4361-bcf7-54b45c967943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
            "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
            "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
            "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
            "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
            "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Alpha, Gamma: (0.3111111111111111, 1.788888888888889)\n",
            "Best Coherence: 0.63347303813736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mergin translated patch ( we did translation 1000  by 1000 in order to prevent system fails )\n",
        "\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: List the Files and Sort\n",
        "import os\n",
        "folder_path = \"/content/drive/MyDrive/Batch1-50\"\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))  # Sorting in the desired order.\n",
        "\n",
        "# Step 3: Merge the Files using Pandas\n",
        "import pandas as pd\n",
        "dfs = []  # List to store dataframes\n",
        "for file in files:\n",
        "    df = pd.read_csv(os.path.join(folder_path, file))\n",
        "    dfs.append(df)\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Step 4: Save the Merged DataFrame\n",
        "output_path = \"/content/drive/MyDrive/Batch1-50\"\n",
        "merged_df.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "crIKGbYcAmRV",
        "outputId": "b0191f6c-77f9-42be-e038-db0d82478a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5729edeebeba>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Batch1-50\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sorting in the desired order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 3: Merge the Files using Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-5729edeebeba>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Batch1-50\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sorting in the desired order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 3: Merge the Files using Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}